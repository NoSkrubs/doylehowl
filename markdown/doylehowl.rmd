---
title: "Doyle Howl"
author: "ADW1"
date: "Early 2018"
output:
  md_document:
    toc: true
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# libraries
library(tidyverse)
library(RPostgres)
library(DBI)
library(lubridate)
library(stringi)
library(stringr)
library(ggthemes)
library(tm)
library(tidytext)
library(wordcloud2)

# connect to pg
con <- dbConnect(RPostgres::Postgres())

# read in raw data
raw_data <- dbReadTable(con, "doyle_howl_data")

# some furhter data prep
data <- raw_data %>%
  mutate(
      week = floor_date(date, "week")
    , day  = floor_date(date, "day")
    , hour = hour(date) + minute(date) / 60
    , wday = wday(date) + hour / 24
    , text = case_when(
          blog_name == "homers-smut"    ~ paste(question, content)
        , blog_name == "reed-emissions" ~ paste(content)
        , blog_name == "reedrelieves"   ~ paste(content)
    )
    , page = case_when(
          blog_name == "homers-smut"    ~ "Reed Releases"
        , blog_name == "reed-emissions" ~ "Reed Emissions"
        , blog_name == "reedrelieves"   ~ "Reed Relieves" 
    )
    , text = stri_enc_toutf8(text)
    , text = str_replace_all(text,"â€™","'")
  )
```

An investigation into the murmurs, musings, and mentions from a memory long forgotten. 

***

# Introduction 

## Gallon-guzzling technique

It was Dec 6th, 2012 at exactly twenty-seven minutes past midnight that the first howl was heard. Fourteen days before the end of the world, a precursor to what would become confessional apocalypse began. Somewhere, someone, had collected a brief anonymous testimony and posted it on the popular blogging site tumblr.com. The text of this first spark of what would be a raging emotional inferno was simply,

> I know you are leaving Reed forever, but you will never leave my heart. DAT GALLON-GUZZLING TECHNIQUE

A forlorn exaltation into the cyber-sphere, a weary lover lamenting the invetible departure of their sexual compatriot; and - to add emphasis, a praise of fallacious talent. 

> DAT GALLON-GUZZLING TECHNIQUE

The first of thousands confessions to follow, all produced anonymously, all managed by a shadowy few secret keepers who would come and go over the proceeding months. In total three incarnations of the service would be wrought over a course of aproximately 18 months, from winter 2012 through the early autumn 2014. 11,485 posts would be made across the three Tumblr pages, which to this day still rest as a memorial to the emotional milue of a point in time; and for many, a continuing source of Google-able nostalgia, and embarassment.

```{r echo=FALSE, fig.height=5, fig.width=10}
title <- "Fig 1: count of posts per week"

  data %>%
  group_by(week, page) %>%
  summarize(
    count = n()
  ) %>%
  ggplot(aes(x = week, y = count, fill = page)) +
    geom_col(alpha = 0.7) +
    xlab("Week") + 
    ylab("Number of posts") + 
    labs(fill = "Page") +
    ggtitle(title) +
    theme_few(base_size = 14)
```

With this memorial though, we can over the distance of time look back at those thoughts, those memories, those incarnations, and begin to understand through the lens of data and analysis what themes, trends, and cultural shifts occured over those 18 months. Although what comes to follow is by no means exhaustive, it is a first pass at what may be a rich source of information at a raw point in time in the lives of students at the Reed College, in Portland, OR.

## The right to forget

For the sake of privacy, all names have been removed from the quoted posts. Furthermore, the raw data of this analysis, and the location of the pages, is intentionally removed from this analysis and Github page. As we will all begin to see, some things we forget should remain forgotten

***

# A brief history of time

## Reed Releases

```{r echo = FALSE}
data(stop_words)
all_words <- data %>%
  filter(page == "Reed Relieves") %>%
  select(text) %>%
  unnest_tokens(word, text, to_lower = TRUE) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE)

wordcloud2(data = all_words)
```


## Reed Emissions

## Reed Relieves

***

# The frequentist approach

## Posting patterns

```{r echo=FALSE, fig.height=5, fig.width=10}
title <- "Figure 2: density of posts by hour in day"

  data %>%
  ggplot(aes(x = hour, fill = blog_name)) +
    geom_density(alpha = 0.5, color = rgb(0,0,0,0)) +
    labs(fill = "Page") +
    ggtitle(title) +
    theme_few(base_size = 14)
```


```{r echo=FALSE, fig.height=5, fig.width=10}
title <- "Figure 2: density of posts by hour in day"

  data %>%
  ggplot(aes(x = hour, fill = blog_name)) +
    geom_density(alpha = 0.5, color = rgb(0,0,0,0)) +
    labs(fill = "Page") +
    ggtitle(title) +
    theme_few(base_size = 14)
```


## Common words

## Variation in language over time

## Use of gender

***

# A sentimental journey

## A basic sentiment pass

### The days we wept

### The days we laughed

## What was triggering

### The horrors we held

### #MeToo

***

# The gaps

## All the likes we can not see

***
# Appendix

## About me

My name is Andrew. 

## The code

The code blocks below detail how the analysis above was performed

### Data preperation

Extracted with `tumblr_utils.py`. This script downloaded the background JSONs for each blog post. The R script below extracts the necessary data from the JSONs and combines into a single table. This table is saved onto the local computer for future analysis and extraction.

```{r eval=FALSE}
# Libraries
library(tidyverse)
library(jsonlite)
library(lubridate)
library(rvest)

strip_html <- function(s) {
    html_text(read_html(s))
}

# list of files
files <- c(
   list.files("../tumblr-utils/homers-smut.tumblr.com/json/"    , full.names = TRUE)
 , list.files("../tumblr-utils/reed-emissions.tumblr.com/json/" , full.names = TRUE)
 , list.files("../tumblr-utils/reedrelieves.tumblr.com/json/"   , full.names = TRUE)
)
length(files)

# empty file
data <- tibble()

# extract json and create data frame
for (file in files) {
  json <- fromJSON(read_lines(file))
  
  question <- ifelse(is.null(json$question), paste0(""), json$question)
  summary  <- ifelse(is.null(json$summary), paste0("") , json$summary)
  content  <- ifelse(
      is.null(json$trail$content_raw)
    , paste0("")
    , str_replace_all(strip_html(json$trail$content_raw), "\n", " ")
  )

  jsonTibble <- tibble(
      id        = as.character(json$id)
    , blog_name = json$blog_name
    , date      = with_tz(ymd_hms(json$date), "America/Los_Angeles")
    , content   = content
    , question  = question
    , summary   = summary
    , short_url = json$short_url
    , post_type = json$type
  )

  data <- data %>% bind_rows(jsonTibble)
}

# Save data to local postgres db
library(RPostgres)
library(DBI)

con <- dbConnect(RPostgres::Postgres())
dbWriteTable(con, "doyle_howl_data", data, overwrite = TRUE)

# check it there
dbGetQuery(con, "SELECT COUNT(*) FROM doyle_howl_data;")
```

### Basic statistics



```{r}
library()

```


###
