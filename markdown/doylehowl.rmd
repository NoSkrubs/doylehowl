---
title: "Doyle Howl"
author: "Andrew Watson"
output:
  html_document:
    fig_caption: yes
    keep_md: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo       = FALSE)
knitr::opts_chunk$set(warning    = FALSE)
knitr::opts_chunk$set(message    = FALSE)
knitr::opts_chunk$set(error      = FALSE)
knitr::opts_chunk$set(fig.height = 4)
knitr::opts_chunk$set(fig.width  = 8)
```


```{r}
## LIBRARIES
# Core data manipulation libraries
library(tidyverse) 
library(lubridate) # dates
library(stringi)   # text
library(stringr)   # text
library(scales)    # number formating

# Database libraries
library(RPostgres)
library(DBI)

# Text analysis libraries
library(tm)
library(tidytext)
library(SnowballC)
library(lexicon)  

# Data visualization libraries
library(ggthemes)
library(wordcloud)
library(kableExtra)
library(knitr)
library(RColorBrewer)
library(ggrepel)

# connect to pg on heroku
connection <- read_file("connection.string")
url <- httr::parse_url(connection)
con <- dbConnect(
   RPostgres::Postgres()
  ,dbname   = url$path
  ,host     = url$hostname
  ,port     = url$port
  ,user     = url$username
  ,password = url$password
)

# local connection
#con <- dbConnect(RPostgres::Postgres())

# read in raw data
raw_data <- dbReadTable(con, "doyle_howl_data")

# some further data prep
data <- raw_data %>%
  mutate(
      week  = date(floor_date(date, "week"))
    , day   = date(floor_date(date, "day"))
    , month = date(floor_date(date, "month"))
    , hour  = hour(date) + minute(date) / 60
    , wday  = wday(date)
    , raw_text = case_when(
          blog_name == "homers-smut"    ~ paste(question, content)
        , blog_name == "reed-emissions" ~ paste(content)
        , blog_name == "reedrelieves"   ~ paste(content)
        , blog_name == 'rerereves'      ~ paste(content)
    )
    , page = case_when(
          blog_name == "homers-smut"    ~ "Reed Releases"
        , blog_name == "reed-emissions" ~ "Reed Emissions"
        , blog_name == "reedrelieves"   ~ "Reed Relieves"
        , blog_name == 'rerereves'      ~ "Re-Rereves"
    )
    , page = factor(page, levels = c("Reed Releases", "Reed Emissions", "Reed Relieves", "Re-Rereves"), ordered = T)
    , text = stri_enc_toutf8(raw_text)
    , text = str_replace_all(text,"’","'")
    , text = str_replace_all(text, "[[:digit:]]", "")
    , text = str_replace_all(text,"\u2019s|'s","")
  )

# filter out Re-Rereves
data <- data %>% filter(page != 'Re-Rereves')

# Bring in Lexicons
data("stop_words")
data("profanity_google")
data("freq_first_names")
data("freq_last_names")

# cut the names down a bit to grab common ones
freq_names <- freq_first_names %>% 
  top_n(1200, n) %>%  # roughly top 20%
  select(name = Name) %>% 
  bind_rows(
    freq_last_names %>% 
      top_n(3000, n) %>%  # roughly top 20%
      select(name = Surname)
  ) %>% 
  distinct()

# Extract data on words, frequency summary
posts_words <- data %>%
  select(page, text, id) %>%
  group_by(page, id) %>%
  unnest_tokens(word, text, to_lower = TRUE) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  mutate(stem = wordStem(word, language = "english")) %>%
  group_by(page, stem, id) %>%
  summarize(
      word = first(word)
    , freq = sum(n)
  ) %>%
  ungroup() %>%
  mutate(
      is_profane = word %in% str_to_lower(profanity_google)
    , is_name    = word %in% str_to_lower(freq_names$name)
  )

# join on a tw / cw flag
posts_triggers <- posts_words %>% 
  filter(word %in% c('tw','cw')) %>% 
  transmute(
    id,
    post_is_triggering = TRUE
  )

# join on the time stamps for the posts
posts_tstamps <- data %>% 
  select(
     id
    ,date
    ,week:wday
  )

# combine
posts_words <- posts_words %>% 
  left_join(posts_triggers) %>% 
  mutate(
    post_is_triggering = coalesce(post_is_triggering, FALSE)
  ) %>% 
  left_join(posts_tstamps)

# Summarize at page level
page_words <- posts_words %>% 
  group_by(
     page
    ,stem
    ,word
  ) %>% 
  summarize(
    freq = sum(freq)
  ) %>% 
  group_by(
     page
    ,stem
  ) %>% 
  summarize(
     word = first(word, desc(freq)) # select common word by stem
    ,freq = sum(freq)
  ) %>% 
  ungroup() %>%
  mutate(
      is_profane = word %in% str_to_lower(profanity_google)
    , is_name    = word %in% str_to_lower(freq_names$name)
  )
```

An investigation into the murmurs, musings, and mentions from a memory long forgotten. See the full sized blog [here](http://blakiseskream.github.io/doylehowl)

***

# Introduction 

## Gallon-guzzling technique

It was Dec 6th, 2012 at exactly twenty-seven minutes past midnight that the first howl was heard. Fourteen days before the end of the world, a precursor to what would become confessional apocalypse began. Somewhere, someone, had collected a brief anonymous testimony and posted it on the popular blogging site tumblr.com. The text of this first spark of what would be a raging emotional inferno was simply,

> I know you are leaving Reed forever, but you will never leave my heart. DAT GALLON-GUZZLING TECHNIQUE

A forlorn exaltation into the cyber-sphere, a weary lover lamenting the invetible departure of their sexual compatriot; and - to add emphasis, a praise of fallacious talent. 

> DAT GALLON-GUZZLING TECHNIQUE

The first of thousands confessions to follow, all produced anonymously, all managed by a shadowy few secret keepers who would come and go over the proceeding months. In total three incarnations of the service would be wrought over a course of aproximately 18 months, from winter 2012 through the early autumn 2014. `r nrow(data)` posts would be made across the three Tumblr pages, which to this day still rest as a memorial to the emotional milieu of a point in time; and for many, a continuing source of Google-able nostalgia, and embarassment.

```{r}
profane_words <- page_words %>%
  filter(is_profane) %>%
  ungroup() %>%
  group_by(word) %>%
  summarize(freq = sum(freq))

wordcloud(
    words = profane_words$word
  , freq = log(profane_words$freq)
  , rot.per = 0
  , fixed.asp = FALSE
  , random.order = FALSE
  , min.freq = 0
  , colors = rev(brewer.pal(11, name = "Dark2"))
)
```
  
<span style='font-size: small;'>*Most commonly used profanity words in posts. Sizes are scaled to log of frequency*</span>

With this memorial though, we can over the distance of time look back at those thoughts, those memories, those incarnations, and begin to understand through the lens of data and analysis what themes, trends, and cultural shifts occured over those 18 months. Although what comes to follow is by no means exhaustive, it is a first pass at what may be a rich source of information at a raw point in time in the lives of students at the Reed College, in Portland, OR.

## The right to forget

For the sake of privacy, all names have been removed from the quoted posts. Furthermore, the raw data of this analysis, and the location of the pages, is intentionally removed from this analysis and Github page. As we will all begin to see, some things we forget should remain forgotten.

***
# A brief history of time

```{r fig.height=4, fig.width=8}
# PLOT: posts by month
data %>%
  group_by(month, page) %>%
  count() %>%
ggplot(aes(x = month, y = n, fill = page)) +
  ggtitle("Posts by month") +
  geom_col() +
  xlab("Month") + 
  ylab("Number of posts") + 
  labs(fill = "Page") +
  theme_few(base_size = 12) +
  scale_x_date(date_labels = format("%b-%Y"), date_breaks = "3 months") +
  scale_y_continuous(labels = comma) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_few()
```

```{r include=FALSE}
# Average days per week posts were made
avg_days_per_week <- data %>%
  distinct(week, day) %>%
  group_by(week) %>%
  count() %>%
  ungroup() %>%
  summarize(
    avg = mean(n)
  ) %>% 
  pull(avg)

# number of total words
total_number_of_words <- data %>%
  select(text) %>%
  unnest_tokens(word, text, to_lower = TRUE) %>%
  nrow() %>% 
  comma()
```

College confession pages were nothing new or unique to Reed or the broader internet in winter 2012. Reed had, over the years, multiple forms of anonymous postings and public internet forums. The Reed College Missed Connections were a popular form of communicating anonymously into the ether. Sent out weekly to the entire student body, the Missed Connections usually had about 10 anonymous blurbs

Although called different things, and run by different people, all three pages mantained a simple structure. Anonymous posts were submitted to the Tumblr pages via the questions feature. On average `r round(avg_days_per_week)` days out of the week the posts would be copied by administrators of the various pages (admins) and pasted onto the Tumblr and a Facebook page or group. A few early rules of conduct prevailed,

  - If a person was named in a post, the admins would message that person directly (usually on Facebook) and ask for their consent to post
  - Any posts showing troubling or triggering content were branded with a "TW:" tag
  - When people had issues with a post, or comment, they would message the admins Facebook page, and the admins would take the comment or post down
  
Generally however, the admins themselves remained anonymous if not elusive. Hidden by the identify of the Facebook page, the rules, workings, and understandings of what was and was not posted remained opaque. Rumor, questions, and a sense of arbitrary determination surrounded much of what was and was not allowed to be posted on the pages. Regardless, there was effectively very little limitation on the content of what could be posted.

> RE 2346: im one of the guys that throws the fish around at the fish market in seattle but what you dont know about me is that i killed a kid in 2003

The Facebook page is where the action would happen. People would comment on the posts, respond to each other, and often times spiral into a vitriolic, viscious, and viral debate. One such thread, "the dread thread", notoriously gained +300 comments before it was finally removed.

> RR 5059: What’s the best way to inform a white person that their dreads are gross (both physically dirty and racist)?

This analysis, however, focuses primarily on the anonymous posts themselves. The vast majority of them written by anonymous students in a solitary vacumm, `r total_number_of_words` words written and submitted by hundreds of ghost writers that will likely remain anonymous for the rest of time.

```{r}
# info about words in post
words_summary <- page_words %>% 
  group_by(Page = page) %>% 
  summarize(
      'Unique words'= n_distinct(word)
    , '% profane'   = (sum(ifelse(is_profane, freq, 0)) / sum(freq))
    , '% names'     = (sum(ifelse(is_name, freq, 0)) / sum(freq))
  )

# info at the post level 
posts_summary <- posts_words %>% 
  group_by(id, Page = page) %>% 
  summarize(
      freq = sum(freq)
    , post_is_triggering = sum(post_is_triggering) > 0
  ) %>% 
  group_by(Page) %>% 
  summarize(
      "Mean post length"      = mean(freq)
    , "# of triggering posts" = sum(post_is_triggering)
  )

# pull together
page_stats <- data %>% 
  group_by(Page = page) %>% 
  summarize(
      "First post" = format(min(day), '%B %d, %Y')
    , "Last post"  = format(max(day), '%B %d, %Y')
    , "Posts"      = n()
  ) %>% 
  left_join(posts_summary) %>% 
  left_join(words_summary) %>% 
  select(
     Page:`Mean post length`
    ,`Unique words`
    ,`# of triggering posts`
    ,`% profane`
    ,`% names`
  )

# output
page_stats %>%
  mutate_at(vars(`% profane`, `% names`), funs(percent(., accuracy = 0.1))) %>%
  mutate_if(is.numeric, comma) %>% 
  kable("html", escape = F, align = c('l','l','l', rep('r', ncol(page_stats) - 3))) %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

## Reed Releases

Reed releases was the first, with `r nrow(filter(data, blog_name == 'Reed Releases'))` total posts, it was also the shortest lived lasting exactly three months, from its first post to its signing off notifcation in February 2013. Reed Releases could also be considered the "purest" of the three incarnations as it did not have ample time to mature and fester. In the sentiment analysis below we will see that it was also the most positive of the pages. Also, notably, Reed Releases didn't have any posts that were formally tagged with a trigger warning.

```{r}
releases_words <- page_words %>% filter(page == "Reed Releases")
wordcloud(
    words = releases_words$word
  , freq = releases_words$freq
  , max.words = 200
  , fixed.asp = FALSE
  , random.order=FALSE
  , rot.per=0
  , colors=brewer.pal(11, name = "Dark2")
)
```

Reed Releases, like the weekly Missed Connections, was focused more on the 1 on 1 interactions that students would have with each other in passing. Posts were often directed at named students, and were confessions of affection, appreciation, or love for one another. This positive appeal is perhaps what drew students to Reed Releases originally, it was a place to see the anamorous outpouring of the college's soul.

> I can't think of anything else except the way you held me. You electrify me, from the bottom of my feet to the flower in my hair. C'mon, let's get our hearts thumping in time and space once again. What do we have to lose?

The gensis of Reed Releases was also well timed. The page start posting on `r page_stats %>% filter(Page == 'Reed Releases') %>% pull('First post')`. The day after the [last day of classes](https://www.reed.edu/registrar/pdfs/reed-academic-calendar-2012-2013.pdf) for the fall semseter in 2012. Although it was on the At Reed the last day of classes for the fall semester is normally on the Wednesday preceding finals week. This is so that the Thursday and Friday

```{r}
data %>% 
  filter(page == 'Reed Releases') %>% 
  count(day) %>% 
  mutate(
    time_category = case_when(
       day < '2012-12-06' ~ 'Fall Semester'
      ,day < '2012-12-10' ~ 'Reading Week'
      ,day < '2012-12-14' ~ 'Finals'
      ,day < '2013-01-21' ~ 'Winter Break'
      ,day < '2013-01-28' ~ 'Paideia'
      ,TRUE               ~ 'Spring Semester'
    )
  ) %>% 
ggplot(aes(x = day, y = n, fill = time_category)) + 
  geom_col() +
  theme_few(base_size = 18) +
  xlab('Day') +
  ylab('# of posts') +
  ggtitle("Reed Releases posts by day") +
  labs(fill = "")
```


## Reed Emissions

Reed Emissions was the second.

```{r echo=FALSE, fig.height=4, fig.width=7}
emissions_words <- page_words %>% filter(page == "Reed Emissions")
wordcloud(
    words = emissions_words$word
  , freq = emissions_words$freq
  , max.words = 200
  , fixed.asp = FALSE
  , random.order=FALSE
  , rot.per=0
  , colors=brewer.pal(11, name = "Dark2")
)
```

## Reed Relieves

```{r echo=FALSE}
relieves_words <- page_words %>% filter(page == "Reed Relieves")
wordcloud(
    words = relieves_words$word
  , freq = relieves_words$freq
  , max.words = 200
  , fixed.asp = FALSE
  , random.order=FALSE
  , rot.per=0
  , colors = rev(brewer.pal(11, name = "Dark2"))
)
```

***

# A sentimental journey

Using simple word based sentiment analysis we can begin to see the mood in the language shift over time. The posts were often volatile, with some revealing deep personal horrors and others expressions of pure joy. Peoples memories of the best and worst times of their lives were hurridly typed into the cybersphere for the college to enjoy, discuss, debate, and more often than not - ridicule. A sentiment based aggregation of these posts can reveal that emotional volatility that so permeated the school during those years.

```{r include=FALSE}
create_sentiment_by_group <- function(words_df, ...) {
  # combine all sentiments into single df
  words_sentiments <- words_df %>% 
    left_join(
      get_sentiments(lexicon = 'afinn') %>% 
        rename(
          afinn_score = score
        )
    ) %>% 
    left_join(
      get_sentiments(lexicon = 'bing') %>% 
        rename(
          bing_sentiment = sentiment
        )
    ) %>% 
    left_join(
      # this contains multiple sentiments per word
      get_sentiments(lexicon = 'nrc') %>%
        mutate(value = 1) %>% 
        spread(key = sentiment, value, fill = 0) 
    ) 
  
  # for taking weighted average
  calc_sentiment <- function(sentiment, freq){
      calc <- sum(sentiment * freq, na.rm = T) / 
           sum(if_else(!is.na(sentiment), freq, 0L))
      
      return(calc)
  }
  
  # collapse the other sentiment calculators into positive vs negative -1 to 1
  # then summarize results by grouping vars
  sentiment_by_group <- words_sentiments %>% 
    mutate(
       bing_score = if_else(bing_sentiment == 'positive', 1, -1)
      ,nrc_score  = positive + negative * -1 #this will result in many 0s btw
    ) %>%
    group_by(...) %>% 
    summarize(
        afinn_n     = sum(ifelse(!is.na(afinn_score), freq, 0))
      , bing_n      = sum(ifelse(!is.na(bing_score), freq, 0))
      , nrc_n       = sum(ifelse(!is.na(nrc_score), freq, 0))
      , afinn_score = calc_sentiment(afinn_score, freq)
      , bing_score  = calc_sentiment(bing_score, freq)
      , nrc_score   = calc_sentiment(nrc_score, freq)
      , n_words     = sum(freq)
    ) %>% 
    mutate(
       afinn_pct = afinn_n / n_words
      ,bing_pct  = bing_n / n_words
      ,nrc_pct   = nrc_n / n_words
    )
}

# create sentiment by page
sentiment_by_page <- create_sentiment_by_group(page_words, page)
```

Below is a comparison of different sentiment scoring methods grouping words into a positive vs negative linear scale (-1 to +1).Of all three methodologies, the EmoLex one from Saif Mohammad and Peter Turney was the most thorough, identifying `r percent(mean(sentiment_by_page$nrc_pct))` of words on average per page.

Reed Releases clearly comes across as more positive vs Reed Emissions and its compatriot Reed Relieves. Perhaps this indicates that as the volume of posts increased over time so did the negative sentiment, or perhaps it is evidence of a general souring of virtual mood over the timer period. Either way, the sentiment transition is stark.

```{r}
sentiment_by_page %>%
  transmute(
     Page       = page
    ,"AFINN"    = afinn_score %>% round(digits = 2)
    ,"Bing"     = bing_score %>% round(digits = 2)
    ,"EmoLex"   = nrc_score %>% round(digits = 2)
  ) %>% 
  kable("html") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, position = "left") #%>% 
  # footnote(
  #    general = "Sentiment analyzers used are, Finn Årup Nielsen (AFINN), Bing Liu and collaborators (Bing), and EmoLex from Saif Mohammad and Peter Turney"
  # )
```

When broken down by time, certain trends begin to come through. Using the AFINN methodology the steady decline in sentiment per post in Reed Emissions leading up to Renn Fayre 2013 is seen clearly. A similar but less discernable decline is also seen in Reed Relieves.

```{r fig.height=4, fig.width=8}
renn_fayre_2013 <- ymd(20130503)
renn_fayre_2014 <- ymd(20140502)

# create sentiment by post, then aggregate the post means
sentiment_by_post_by_week <- create_sentiment_by_group(posts_words, page, week, id) %>% 
  group_by(page, week) %>% 
  summarize(
     nrc_post_mean   = mean(nrc_score, na.rm = T)
    ,afinn_post_mean = mean(afinn_score, na.rm = T)
    ,bing_post_mean  = mean(bing_score, na.rm = T)
    ,n_posts = n_distinct(id)
  )

sentiment_by_post_by_week %>% 
  ggplot(aes(
      x = week
    , y = afinn_post_mean
    , group = page
    , color = page
  )) +
  geom_point(aes(size = n_posts), alpha = 0.5) +
  theme_few(base_size = 12) +
  scale_x_date(date_labels = format("%b-%Y"), date_breaks = "3 months") +
  scale_y_continuous(breaks = seq(-1.5, 1.5, 0.5)) +
  scale_color_few() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_vline(xintercept = renn_fayre_2013, color = 'grey', alpha = 0.5) +
  geom_vline(xintercept = renn_fayre_2014, color = 'grey', alpha = 0.5) +
  ggplot2::annotate(
      x = renn_fayre_2013
    , y = 1
    , geom = 'text'
    , label = "Renn Fayre 2013"
    , hjust = -0.1
    , alpha = 0.8
    , size = 3
  ) +
  ggplot2::annotate(
      x = renn_fayre_2014
    , y = 1
    , geom = 'text'
    , label = "Renn Fayre 2014"
    , hjust = -0.1
    , alpha = 0.8
    , size = 3
  ) +
  guides(size = 'none') +
  labs(
      color = ""
    , x = ""
    , y = "sentiment"
  ) +
  ggtitle(
     "Mean post sentiment by week"
    ,subtitle = 'AFINN score'
  )
```

This pattern would play nicely into Reed cultural narratives around Spring Crises, Junior Quals, and the stress of senior theses. We can start to verify this by observing the frequencey of posts with mentions of the stem or word 'thesis', 'midterm' and or 'qual'.

```{r fig.height=4, fig.width=8}
# create sentiment by post, then aggregate the post means
sentiment_by_post_by_day <- create_sentiment_by_group(posts_words, page, day, id) %>% 
  group_by(page, day) %>% 
  summarize(
     nrc_post_mean   = mean(nrc_score, na.rm = T)
    ,afinn_post_mean = mean(afinn_score, na.rm = T)
    ,bing_post_mean  = mean(bing_score, na.rm = T)
    ,n_posts = n_distinct(id)
  )

# grab # of mentions of 'thesis' by day
thesis_frequency <- posts_words %>% 
  filter(
      stem %in% c('thesis', 'qual', 'midterm') 
    | word == 'thesis'
  ) %>% 
  group_by(day, page) %>%
  summarize(
     n_thesis_posts = n_distinct(id)
  ) %>% 
  left_join(sentiment_by_post_by_day)

thesis_frequency %>% 
  #filter(n_posts > 10) %>% 
  ggplot(aes(
      x = day
    , y = n_thesis_posts / n_posts
    , group = page
    , color = page
    , size = n_posts
  )) +
  geom_point(alpha = 0.5) +
  theme_few(base_size = 12) +
  scale_x_date(date_labels = format("%b-%Y"), date_breaks = "3 months") +
  scale_y_continuous(label = percent) +
  scale_color_few() +
  coord_cartesian(ylim = c(0, 0.25)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_vline(xintercept = renn_fayre_2013, color = 'grey', alpha = 0.5) +
  geom_vline(xintercept = renn_fayre_2014, color = 'grey', alpha = 0.5) +
  ggplot2::annotate(
      x = renn_fayre_2013
    , y = 0.2
    , geom = 'text'
    , label = "Renn Fayre 2013"
    , hjust = -0.15
    , alpha = 0.8
    , size = 3
  ) +
  ggplot2::annotate(
      x = renn_fayre_2014
    , y = 0.2
    , geom = 'text'
    , label = "Renn Fayre 2014"
    , hjust = -0.1
    , alpha = 0.8
    , size = 3
  ) +
  guides(size = 'none') +
  labs(
      color = ""
    , x = ""
    , y = "share"
  ) +
  ggtitle("Share of posts with 'thesis', 'midterm' or 'qual' by day")
```

The trend is mostly how we would expect. The days leading up to and the week of Renn Fayre see the highest frequency of posts mentioning these stressful, then potentially joyous activities. This is particularly pronounced in the data for Reed Emissions, which sees a steady upward trend of frequencies, mimicing the same trend in declining sentiment.

This, however, can be a sell fulfilling prophecy. Taking a purely exploratory approach we can tease apart the terms mentioned most frequently in those days with the lowest sentiment.

```{r}
# perform counts of words most mentioned in low sentiment days
low_sentiment_days <- sentiment_by_post_by_day %>% 
  filter(n_posts > 10) %>% 
  ungroup() %>% 
  top_n(10, -afinn_post_mean) %>% 
  inner_join(posts_words) %>% 
  group_by(word) %>% 
  summarize(
    freq = sum(freq)
  )

wordcloud(
    words         = low_sentiment_days$word
  , freq          = low_sentiment_days$freq
  , rot.per       = 0
  , fixed.asp     = FALSE
  , random.order  = FALSE
  , colors        = brewer.pal(10, name = "RdGy")
  #, size          = 10
)
```

And compare then the same with the high sentiment days

```{r}
# perform counts of words most mentioned in low sentiment days
high_sentiment_days <- sentiment_by_post_by_day %>% 
  filter(n_posts > 10) %>% 
  ungroup() %>% 
  top_n(10, afinn_post_mean) %>% 
  inner_join(posts_words) %>% 
  group_by(word) %>% 
  summarize(
    freq = sum(freq)
  )

wordcloud(
    words         = high_sentiment_days$word
  , freq          = high_sentiment_days$freq
  , rot.per       = 0
  , fixed.asp     = FALSE
  , random.order  = FALSE
  , colors        = brewer.pal(10, name = "Dark2")
  #, size          = 10
)
```

Then to truly visualize the difference, observe the words that are not shared across the two sets. 

We can start on a positive note with the high sentiment days. 

```{r}
high_sentiment_only <- high_sentiment_days %>% anti_join(low_sentiment_days %>% distinct(word))

wordcloud(
    words         = high_sentiment_only$word
  , freq          = high_sentiment_only$freq
  , rot.per       = 0
  , fixed.asp     = FALSE
  , random.order  = FALSE
  , min.freq      = 2 
  , colors        = brewer.pal(10, name = "Dark2")
  #, size          = 10
)
```

Now contrast that with the words we see on the bottom 10 sentiment days that are *not* present in words seen in the top 10 sentiment days.

```{r}
low_sentiment_only <- low_sentiment_days %>% anti_join(high_sentiment_days %>% distinct(word))

wordcloud(
    words         = low_sentiment_only$word
  , freq          = low_sentiment_only$freq
  , rot.per       = 0
  , fixed.asp     = FALSE
  , random.order  = FALSE
  , colors        = brewer.pal(10, name = "RdGy")
  , min.freq      = 2 
)
```

The difference is striking - and incredibly revealing. It's the stuff of life that sets them apart, and most importantly, the stuff of life related to trauma. The fact that not a single post labeled with a trigger warning was located in the top 10 happiest days on these pages casts a new light on the content, and requires us to go deeper.

It's not theses, or midterms, or quals that appear to dictate the mood of the populous. It's trauma, assault, sex, mental health, and identity. The record of that trauma sits in these posts like a mass grave. Through the lens of data we can hope to exhume these years old traumas, and attempt to see how deep that trauma lives.

***

## The triggers

```{r}
trigger_posts <- posts_words %>% 
  filter(post_is_triggering) %>% 
  group_by(word) %>% 
  summarize(
    freq = sum(freq)
  ) %>% 
  filter(! word %in% c('cw','tw'))
  
wordcloud(
    words         = trigger_posts$word
  , freq          = trigger_posts$freq
  , rot.per       = 0
  , fixed.asp     = FALSE
  , random.order  = FALSE
  , colors        = brewer.pal(10, name = "RdGy")
)
```


### The horrors we held

### #MeToo

***

# The gaps

## All the likes we can not see

***
# Appendix

## About me

My name is Andrew. 

## Basic statistics

### Day of week posts

```{r}
# day in week
data %>%
  group_by(page, wday) %>%
  count() %>%
ggplot(aes(x = wday, y = n, fill = page)) +
  geom_col(alpha = 0.5) +
  facet_wrap(~page) +
  guides(fill = FALSE) +
  ggtitle("Count of posts by day in week") +
  theme_few(base_size = 14) +
  scale_fill_few()
```


### Hour of day

```{r}
data %>%
  ggplot(aes(x = hour, fill = page)) +
    geom_density(alpha = 0.5) +
    labs(fill = "Page") +
    ggtitle("Density of posts by hour in day") +
    theme_few(base_size = 14) +
    scale_fill_few()
```

### Share of top 100 words

```{r}
page_words %>%
  group_by(page) %>%
  mutate(norm_freq = freq / sum(freq)) %>%
  top_n(100) %>%
  ggplot(aes(x = norm_freq, fill = page)) + 
  geom_density(alpha = 0.5) + 
  scale_x_continuous(label = percent) +
  ggtitle("Share of total words the top 100 words make up") +
  theme_minimal() +
  scale_fill_few()
```


### Words per post

```{r}
posts_words %>% 
  group_by(id, Page = page) %>% 
  summarize(
    freq = sum(freq)
  ) %>% 
  ggplot(aes(x = freq, fill = Page)) +
    geom_density(alpha = 0.5) +
    labs(fill = "Page") +
    ggtitle("Words per post") +
    theme_few(base_size = 14) +
    scale_fill_few() +
    scale_x_log10()
```

## Data preperation

Extracted with `tumblr_utils.py`. This script downloaded the background JSONs for each blog post. The R script below extracts the necessary data from the JSONs and combines into a single table. This table is saved onto the local computer for future analysis and extraction.

```{r eval=FALSE, echo=TRUE}
# Libraries
library(tidyverse)
library(jsonlite)
library(lubridate)
library(rvest)

strip_html <- function(s) {
    html_text(read_html(s))
}

# list of files
files <- c(
   list.files("../tumblr-utils/homers-smut.tumblr.com/json/", full.names = TRUE)
 , list.files("../tumblr-utils/reed-emissions.tumblr.com/json/", full.names = TRUE)
 , list.files("../tumblr-utils/reedrelieves.tumblr.com/json/", full.names = TRUE)
 , list.files("../tumblr-utils/rerereves.tumblr.com/json/", full.names = TRUE)

)
length(files)

# empty file
data <- tibble()

# extract json and create data frame
for (file in files) {
  json <- fromJSON(read_lines(file))
  
  question <- ifelse(is.null(json$question), paste0(""), json$question)
  summary  <- ifelse(is.null(json$summary), paste0("") , json$summary)
  content  <- ifelse(
      is.null(json$trail$content_raw)
    , paste0("")
    , str_replace_all(strip_html(json$trail$content_raw), "\n", " ")
  )

  jsonTibble <- tibble(
      id        = as.character(json$id)
    , blog_name = json$blog_name
    , date      = with_tz(ymd_hms(json$date), "America/Los_Angeles")
    , content   = content
    , question  = question
    , summary   = summary
    , short_url = json$short_url
    , post_type = json$type
  )

  data <- data %>% bind_rows(jsonTibble)
}
```

You can then perform the heroku upload

```{r eval=FALSE}
# Save data to Heroku
library(RPostgres)
library(DBI)

# connect to pg
connection <- read_file("connection.string")
url <- httr::parse_url(connection)

con <- dbConnect(
   RPostgres::Postgres()
  ,dbname   = url$path
  ,host     = url$hostname
  ,port     = url$port
  ,user     = url$username
  ,password = url$password
)

dbWriteTable(con, "doyle_howl_data_temp", data, overwrite = TRUE)
```

## Common words

## Variation in language over time

## Use of gender
