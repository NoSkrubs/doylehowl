---
title: "Doyle Howl"
author: "Andrew Watson"
output:
  html_document:
    fig_caption: yes
    keep_md: yes
    toc: yes
    toc_depth: 2
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo       = FALSE)
knitr::opts_chunk$set(warning    = FALSE)
knitr::opts_chunk$set(message    = FALSE)
knitr::opts_chunk$set(error      = FALSE)
knitr::opts_chunk$set(fig.height = 4)
knitr::opts_chunk$set(fig.width  = 8)

## LIBRARIES
# Core data manipulation libraries
library(tidyverse) 
library(lubridate) # dates
library(stringi)   # text
library(stringr)   # text
library(scales)    # number formating

# Database libraries
library(RPostgres)
library(DBI)

# Text analysis libraries
library(tm)
library(tidytext)
library(SnowballC)
library(lexicon)  

# Data visualization libraries
library(ggthemes)
library(wordcloud)
library(kableExtra)
library(knitr)
library(RColorBrewer)

# connect to pg
connection <- read_file("connection.string")
url <- httr::parse_url(connection)

con <- dbConnect(
   RPostgres::Postgres()
  ,dbname   = url$path
  ,host     = url$hostname
  ,port     = url$port
  ,user     = url$username
  ,password = url$password
)

# read in raw data
raw_data <- dbReadTable(con, "doyle_howl_data")

# some furhter data prep
data <- raw_data %>%
  mutate(
      week  = date(floor_date(date, "week"))
    , day   = date(floor_date(date, "day"))
    , month = date(floor_date(date, "month"))
    , hour  = hour(date) + minute(date) / 60
    , wday  = wday(date)
    , raw_text = case_when(
          blog_name == "homers-smut"    ~ paste(question, content)
        , blog_name == "reed-emissions" ~ paste(content)
        , blog_name == "reedrelieves"   ~ paste(content)
    )
    , page = case_when(
          blog_name == "homers-smut"    ~ "Reed Releases"
        , blog_name == "reed-emissions" ~ "Reed Emissions"
        , blog_name == "reedrelieves"   ~ "Reed Relieves" 
    )
    , page = factor(page, levels = c("Reed Releases", "Reed Emissions", "Reed Relieves"), ordered = T)
    , text = stri_enc_toutf8(raw_text)
    , text = str_replace_all(text,"’","'")
    , text = str_replace_all(text, "[[:digit:]]", "")
    , text = str_replace_all(text,"\u2019s|'s","")
  )

# Extract preliminary data on words
data("stop_words")
data("profanity_google")
data("freq_first_names")
data("freq_last_names")

page_words <- data %>%
  select(page, text) %>%
  group_by(page) %>%
  unnest_tokens(word, text, to_lower = TRUE) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  mutate(stem = wordStem(word, language = "english")) %>%
  group_by(page, stem) %>%
  summarize(
      word = first(word)
    , freq = sum(n)
  ) %>%
  ungroup() %>%
  mutate(
      is_profane = word %in% str_to_lower(profanity_google)
    , is_name    = word %in% str_to_lower(freq_first_names$Name) | word %in% str_to_lower(freq_last_names$Surname)
  ) %>%
  group_by(page)
```

An investigation into the murmurs, musings, and mentions from a memory long forgotten. 

***

# Introduction 

## Gallon-guzzling technique

It was Dec 6th, 2012 at exactly twenty-seven minutes past midnight that the first howl was heard. Fourteen days before the end of the world, a precursor to what would become confessional apocalypse began. Somewhere, someone, had collected a brief anonymous testimony and posted it on the popular blogging site tumblr.com. The text of this first spark of what would be a raging emotional inferno was simply,

> I know you are leaving Reed forever, but you will never leave my heart. DAT GALLON-GUZZLING TECHNIQUE

A forlorn exaltation into the cyber-sphere, a weary lover lamenting the invetible departure of their sexual compatriot; and - to add emphasis, a praise of fallacious talent. 

> DAT GALLON-GUZZLING TECHNIQUE

The first of thousands confessions to follow, all produced anonymously, all managed by a shadowy few secret keepers who would come and go over the proceeding months. In total three incarnations of the service would be wrought over a course of aproximately 18 months, from winter 2012 through the early autumn 2014. **11,485** posts would be made across the three Tumblr pages, which to this day still rest as a memorial to the emotional milieu of a point in time; and for many, a continuing source of Google-able nostalgia, and embarassment.

```{r fig.height=3, fig.width=3}
profane_words <- page_words %>%
  filter(is_profane) %>%
  ungroup() %>%
  group_by(word) %>%
  summarize(freq = sum(freq)) %>% 
  filter(word != 'fuck')

wordcloud(
    words = profane_words$word
  , freq = profane_words$freq
  , rot.per = 0
  , fixed.asp = FALSE
  , random.order = FALSE
  , colors=brewer.pal(11, name = "RdGy")
)
```
  
<span style='font-size: small;'>*Most commonly used profanity words in posts. "Fuck" has been removed to better show the relationships of other words.*</span>

With this memorial though, we can over the distance of time look back at those thoughts, those memories, those incarnations, and begin to understand through the lens of data and analysis what themes, trends, and cultural shifts occured over those 18 months. Although what comes to follow is by no means exhaustive, it is a first pass at what may be a rich source of information at a raw point in time in the lives of students at the Reed College, in Portland, OR.

## The right to forget

For the sake of privacy, all names have been removed from the quoted posts. Furthermore, the raw data of this analysis, and the location of the pages, is intentionally removed from this analysis and Github page. As we will all begin to see, some things we forget should remain forgotten

***
# A brief history of time

```{r fig.height=4, fig.width=8}
# PLOT: posts by month
data %>%
  group_by(month, page) %>%
  count() %>%
ggplot(aes(x = month, y = n, fill = page)) +
  ggtitle("Posts by month") +
  geom_col(alpha = 0.7) +
  xlab("Month") + 
  ylab("Number of posts") + 
  labs(fill = "Page") +
  theme_few(base_size = 12) +
  scale_x_date(date_labels = format("%b-%Y"), date_breaks = "3 months") +
  scale_y_continuous(labels = comma) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r include=FALSE}
# summary stats needed for this section

# Average days per week posts were made
data %>%
  distinct(week, day) %>%
  group_by(week) %>%
  count() %>%
  ungroup() %>%
  summarize(
    avg = mean(n)
  )

# number of total words
data %>%
  select(text) %>%
  unnest_tokens(word, text, to_lower = TRUE) %>%
  nrow()
```

College confession pages were nothing new or unique to Reed or the broader internet in winter 2012. Reed had, over the years, multiple forms of anonymous postings and public internet forums. The Reed College Missed Connections were a popular form of communicating anonymously into the ether. Sent out weekly to the entire student body, the Missed Connections usually had about 10 anonymous blurbs

Although called different things, and run by different people, all three pages mantained a simple structure. Anonymous posts were submitted to the Tumblr pages via the questions feature. Four days out of the week the posts would be copied by administrators of the various pages (admins) and pasted onto the Tumblr and a Facebook page or group. A few early rules of conduct prevailed,

  - If a person was named in a post, the admins would message that person directly (usually on Facebook) and ask for their consent to post
  - Any posts showing troubling or triggering content were branded with a "TW:" tag
  - When people had issues with a post, or comment, they would message the admins Facebook page, and the admins would take the comment or post down
  
Generally however, the admins themselves remained anonymous if not elusive. Hidden by the identify of the Facebook page, the rules, workings, and understandings of what was and was not posted remained opaque. Rumor, questions, and a sense of arbitrary determination surrounded much of what was and was not allowed to be posted on the pages. Regardless, there was effectively very little limitation on the content of what could be posted.

> RE 2346: im one of the guys that throws the fish around at the fish market in seattle but what you dont know about me is that i killed a kid in 2003

The Facebook page is where the action would happen. People would comment on the posts, respond to each other, and often times spiral into a vitriolic, viscious, and viral debate. One such thread, "the dread thread", notoriously gained +300 comments before it was finally removed.

> RR 5059: What’s the best way to inform a white person that their dreads are gross (both physically dirty and racist)?

This analysis, however, focuses primarily on the anonymous posts themselves. The vast majority of them written by anonymous students in a solitary vacumm, 349,000 words written and submitted by hundreds of ghost writers that will likely remain anonymous for the rest of time.

```{r}
page_stats <- data %>% 
  group_by(Page = page) %>% 
  summarize(
      "First post" = format(min(day), '%B %d, %Y')
    , "Last post"  = format(max(day), '%B %d, %Y')
    ,  Count       = n()
  )
  
page_stats %>%
   kable("html") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, position = "left")
```

## Reed Releases

Reed releases was the first, with 604 total posts, it was also the shortest lived lasting exactly three months, from its first post to its signing off notifcation in February 2013. Reed Releases could also be considered the "purest" of the three incarnations as it did not have ample time to mature and fester. In the sentiment analysis below we will see that it was also the most positive of the pages, with the smallest share of trigger warnings and negative words [TODO: VALIDATE]. 

```{r}
releases_words <- page_words %>% filter(page == "Reed Releases")
wordcloud(
    words = releases_words$word
  , freq = releases_words$freq
  , max.words = 200
  , fixed.asp = FALSE
  , random.order=FALSE
  , rot.per=0
  , colors=brewer.pal(11, name = "Dark2")
)
```

## Reed Emissions

```{r echo=FALSE, fig.height=4, fig.width=7}
emissions_words <- page_words %>% filter(page == "Reed Emissions")
wordcloud(
    words = emissions_words$word
  , freq = emissions_words$freq
  , max.words = 200
  , fixed.asp = FALSE
  , random.order=FALSE
  , rot.per=0
  , colors=brewer.pal(11, name = "Dark2")
)
```

## Reed Relieves

```{r echo=FALSE}
relieves_words <- page_words %>% filter(page == "Reed Relieves")
wordcloud(
    words = relieves_words$word
  , freq = relieves_words$freq
  , max.words = 200
  , fixed.asp = FALSE
  , random.order=FALSE
  , rot.per=0
  , colors=rev(brewer.pal(11, name = "Dark2"))
)
```

***

# A sentimental journey

Using simple word based sentiment analysis we can begin to 

```{r include=FALSE}
# combine all sentiments into single df
words_sentiments <- page_words %>% 
  left_join(
    get_sentiments(lexicon = 'afinn') %>% 
      rename(
        afinn_score = score
      )
  ) %>% 
  left_join(
    get_sentiments(lexicon = 'bing') %>% 
      rename(
        bing_sentiment = sentiment
      )
  ) %>% 
  left_join(
    # this contains multiple sentiments per word
    get_sentiments(lexicon = 'nrc') %>%
      mutate(value = 1) %>% 
      spread(key = sentiment, value, fill = 0) 
  )

# collapse the other sentiment calculators into positive vs negative -1 to 1
# then summarize results by page
sentiment_by_page <- words_sentiments %>% 
  mutate(
     bing_score = if_else(bing_sentiment == 'positive', 1, -1)
    ,nrc_score  = positive + negative * -1 #this will result in many 0s btw
  ) %>%
  group_by(page) %>% 
  summarize_at(
      vars(afinn_score, bing_score, nrc_score)
     ,funs(
         sum(. * freq, na.rm = T) / 
         sum(if_else(!is.na(.), freq, 0L))
      )
  )
```

Below is a comparison of different sentiment scoring methods grouping words into a positive vs negative linear scale (-1 to +1). Across all three methodologies Reed Releases clearly comes across as more positive vs Reed Emissions and its compatriot Reed Relieves. Perhaps this indicates that as the volume of posts increased over time so did the negative sentiment, or perhaps it is evidence of a general souring of virtual mood over the timer period. Either way, the sentiment transition is stark.

```{r}
sentiment_by_page %>%
  transmute(
     Page       = page
    ,"AFINN"    = afinn_score %>% round(digits = 2)
    ,"Bing"     = bing_score %>% round(digits = 2)
    ,"EmoLex"   = nrc_score %>% round(digits = 2)
  ) %>% 
  kable("html") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, position = "left") #%>% 
  # footnote(
  #    general = "Sentiment analyzers used are, Finn Årup Nielsen (AFINN), Bing Liu and collaborators (Bing), and EmoLex from Saif Mohammad and Peter Turney"
  # )
```

When we break this out by month we can 



## What was triggering

```{r fig.height=4, fig.width=4}
trigger_posts <- data %>%
  filter(str_detect(str_to_lower(text), "tw:")) %>%
  ungroup() %>%
  select(text) %>%
  unnest_tokens(word, text, to_lower = TRUE) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  mutate(stem = wordStem(word, language = "english")) %>%
  group_by(stem) %>%
  summarize(
      word = first(word)
    , freq = sum(n)
  ) %>%
  ungroup() %>% 
  filter(word != 'tw')
  
wordcloud(
    words         = trigger_posts$word
  , freq          = trigger_posts$freq
  , rot.per       = 0
  , fixed.asp     = FALSE
  , random.order  = FALSE
  , colors        = brewer.pal(10, name = "RdGy")
  , size          = 10
)
```


### The horrors we held

### #MeToo

***

# The gaps

## All the likes we can not see

***
# Appendix

## About me

My name is Andrew. 

## The code

The code blocks below detail how the analysis above was performed

### Data preperation

Extracted with `tumblr_utils.py`. This script downloaded the background JSONs for each blog post. The R script below extracts the necessary data from the JSONs and combines into a single table. This table is saved onto the local computer for future analysis and extraction.

```{r eval=FALSE, echo=TRUE}
# Libraries
library(tidyverse)
library(jsonlite)
library(lubridate)
library(rvest)

strip_html <- function(s) {
    html_text(read_html(s))
}

# list of files
files <- c(
   list.files("../tumblr-utils/reed-releases-jsons/json/"    , full.names = TRUE)
 , list.files("../tumblr-utils/reed-emissions-jsons/json/" , full.names = TRUE)
 , list.files("../tumblr-utils/reed-relieves-jsons/json/"   , full.names = TRUE)
)
length(files)

# empty file
data <- tibble()

# extract json and create data frame
for (file in files) {
  json <- fromJSON(read_lines(file))
  
  question <- ifelse(is.null(json$question), paste0(""), json$question)
  summary  <- ifelse(is.null(json$summary), paste0("") , json$summary)
  content  <- ifelse(
      is.null(json$trail$content_raw)
    , paste0("")
    , str_replace_all(strip_html(json$trail$content_raw), "\n", " ")
  )

  jsonTibble <- tibble(
      id        = as.character(json$id)
    , blog_name = json$blog_name
    , date      = with_tz(ymd_hms(json$date), "America/Los_Angeles")
    , content   = content
    , question  = question
    , summary   = summary
    , short_url = json$short_url
    , post_type = json$type
  )

  data <- data %>% bind_rows(jsonTibble)
}

# Save data to local postgres db
library(RPostgres)
library(DBI)

con <- dbConnect(RPostgres::Postgres())
dbWriteTable(con, "doyle_howl_data", data, overwrite = TRUE)

# check it there
dbGetQuery(con, "SELECT COUNT(*) FROM doyle_howl_data;")
```

### Basic statistics

```{r echo=TRUE}
title <- "Density of posts by day in week"
  data %>%
    group_by(page, wday) %>%
    count() %>%
  ggplot(aes(x = wday, y = n, fill = page)) +
    geom_col(alpha = 0.5, color = rgb(0,0,0,0)) +
    facet_wrap(~page) +
    guides(fill = FALSE) +
    ggtitle(title) +
    theme_few(base_size = 14)

title <- "Density of posts by day in week"
page_words %>%
  group_by(page) %>%
  mutate(norm_freq = freq / sum(freq)) %>%
  top_n(100) %>%
  ggplot(aes(x = norm_freq, fill = page)) + 
  geom_density(alpha = 0.5) + 
  scale_x_continuous(label = percent) +
  ggtitle(title)
```

***

# The frequentist approach

## Posting patterns

```{r echo=FALSE, fig.height=4, fig.width=8}
title <- "Figure 2: density of posts by hour in day"

  data %>%
  ggplot(aes(x = hour, fill = page)) +
    geom_density(alpha = 0.5, color = rgb(0,0,0,0)) +
    labs(fill = "Page") +
    ggtitle(title) +
    theme_few(base_size = 14)
```

## Common words

## Variation in language over time

## Use of gender
